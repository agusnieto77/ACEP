% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/acep_process_chunks.R
\name{acep_process_chunks}
\alias{acep_process_chunks}
\title{Procesamiento de textos en lotes para optimizar memoria}
\usage{
acep_process_chunks(
  texto,
  funcion,
  chunk_size = 1000,
  show_progress = TRUE,
  ...
)
}
\arguments{
\item{texto}{Vector de caracteres con los textos a procesar.}

\item{funcion}{Función de ACEP a aplicar a cada lote. Ejemplos: `acep_clean`,
`acep_token`, `acep_count`, `acep_upos`, etc. Debe ser una función que acepte
un vector de textos como primer argumento.}

\item{chunk_size}{Número de textos por lote. Valores más bajos reducen el consumo
de memoria pero aumentan el tiempo total de procesamiento. Por defecto: 1000.}

\item{show_progress}{Lógico. Si `TRUE`, muestra mensajes informativos sobre el
progreso del procesamiento (qué lote se está procesando). Por defecto: `TRUE`.}

\item{...}{Argumentos adicionales que se pasan directamente a la función especificada
en el parámetro `funcion`. Ejemplo: si `funcion = acep_clean`, puede pasar
`rm_stopwords = TRUE`, `tolower = TRUE`, etc.}
}
\value{
El tipo de resultado depende de la función aplicada:
\itemize{
  \item Si la función retorna un vector, devuelve un vector combinado
  \item Si la función retorna un data frame, devuelve un data frame combinado (rbind)
  \item Si la función retorna una lista, devuelve una lista de listas
}
}
\description{
Divide un vector grande de textos en lotes (chunks) más pequeños y los procesa
secuencialmente aplicando una función de ACEP. Esta estrategia permite analizar
corpus extensos (millones de documentos) sin superar la capacidad de memoria RAM
disponible. La función combina automáticamente los resultados de todos los lotes.
}
\examples{
\dontrun{
# Procesar 10,000 textos con limpieza en lotes de 1000
textos_limpios <- acep_process_chunks(
  texto = corpus_grande,
  funcion = acep_clean,
  chunk_size = 1000,
  rm_stopwords = TRUE
)

# Tokenizar corpus masivo
tokens <- acep_process_chunks(
  texto = corpus_masivo,
  funcion = acep_token,
  chunk_size = 500,
  tolower = TRUE
)

# Contar menciones en corpus grande
diccionario <- c("paro", "huelga", "protesta")
frecuencias <- acep_process_chunks(
  texto = corpus_grande,
  funcion = acep_count,
  chunk_size = 2000,
  dic = diccionario
)
}

}
