<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="es"><head><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><title>Interaccion con modelos Ollama locales y cloud usando Structured Outputs — acep_ollama • ACEP</title><!-- favicons --><link rel="icon" type="image/png" sizes="96x96" href="../favicon-96x96.png"><link rel="icon" type="”image/svg+xml”" href="../favicon.svg"><link rel="apple-touch-icon" sizes="180x180" href="../apple-touch-icon.png"><link rel="icon" sizes="any" href="../favicon.ico"><link rel="manifest" href="../site.webmanifest"><script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"><link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet"><script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet"><link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet"><script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="Interaccion con modelos Ollama locales y cloud usando Structured Outputs — acep_ollama"><meta name="description" content="Funcion para interactuar con modelos de lenguaje usando Ollama.
Soporta tanto modelos locales (ejecutados en tu computadora sin costos)
como modelos cloud de Ollama (modelos grandes como DeepSeek 671B, Qwen3 Coder 480B,
Kimi 1T que se ejecutan en la nube sin necesidad de GPU local).
Utiliza structured outputs para garantizar respuestas en formato JSON que
cumplen con un esquema predefinido."><meta property="og:description" content="Funcion para interactuar con modelos de lenguaje usando Ollama.
Soporta tanto modelos locales (ejecutados en tu computadora sin costos)
como modelos cloud de Ollama (modelos grandes como DeepSeek 671B, Qwen3 Coder 480B,
Kimi 1T que se ejecutan en la nube sin necesidad de GPU local).
Utiliza structured outputs para garantizar respuestas en formato JSON que
cumplen con un esquema predefinido."><meta property="og:image" content="https://agusnieto77.github.io/ACEP/logo.svg"></head><body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">ACEP</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.1.0.9000</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto"><li class="active nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="nav-item dropdown">
  <button class="nav-link dropdown-toggle" type="button" id="dropdown-articles" data-bs-toggle="dropdown" aria-expanded="false" aria-haspopup="true">Articles</button>
  <ul class="dropdown-menu" aria-labelledby="dropdown-articles"><li><a class="dropdown-item" href="../articles/conflictividad_soip.html">Conflictividad laboral en la pesca</a></li>
    <li><a class="dropdown-item" href="../articles/extraccion_de_svo_con_acep.html">Extraer S-V-O con ACEP</a></li>
    <li><a class="dropdown-item" href="../articles/extraccion_palabras_clave.html">Extracción de palabas clave</a></li>
    <li><a class="dropdown-item" href="../articles/limpieza_de_texto_con_acep.html">Limpieza de texto con ACEP</a></li>
    <li><a class="dropdown-item" href="../articles/tokenizar_con_acep.html">Tokenizar con ACEP</a></li>
  </ul></li>
<li class="nav-item"><a class="nav-link" href="../news/index.html">Changelog</a></li>
      </ul><ul class="navbar-nav"><li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json"></form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/agusnieto77/ACEP/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul></div>


  </div>
</nav><div class="container template-reference-topic">
<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">
      <img src="../logo.svg" class="logo" alt=""><h1>Interaccion con modelos Ollama locales y cloud usando Structured Outputs</h1>
      <small class="dont-index">Source: <a href="https://github.com/agusnieto77/ACEP/blob/master/R/acep_ollama.R" class="external-link"><code>R/acep_ollama.R</code></a></small>
      <div class="d-none name"><code>acep_ollama.Rd</code></div>
    </div>

    <div class="ref-description section level2">
    <p>Funcion para interactuar con modelos de lenguaje usando Ollama.
Soporta tanto modelos locales (ejecutados en tu computadora sin costos)
como modelos cloud de Ollama (modelos grandes como DeepSeek 671B, Qwen3 Coder 480B,
Kimi 1T que se ejecutan en la nube sin necesidad de GPU local).
Utiliza structured outputs para garantizar respuestas en formato JSON que
cumplen con un esquema predefinido.</p>
    </div>

    <div class="section level2">
    <h2 id="ref-usage">Usage<a class="anchor" aria-label="anchor" href="#ref-usage"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span><span class="fu">acep_ollama</span><span class="op">(</span></span>
<span>  <span class="va">texto</span>,</span>
<span>  <span class="va">instrucciones</span>,</span>
<span>  modelo <span class="op">=</span> <span class="st">"qwen3:1.7b"</span>,</span>
<span>  schema <span class="op">=</span> <span class="cn">NULL</span>,</span>
<span>  parse_json <span class="op">=</span> <span class="cn">TRUE</span>,</span>
<span>  temperature <span class="op">=</span> <span class="fl">0</span>,</span>
<span>  max_tokens <span class="op">=</span> <span class="fl">4000</span>,</span>
<span>  host <span class="op">=</span> <span class="st">"http://localhost:11434"</span>,</span>
<span>  api_key <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/Sys.getenv.html" class="external-link">Sys.getenv</a></span><span class="op">(</span><span class="st">"OLLAMA_API_KEY"</span><span class="op">)</span>,</span>
<span>  seed <span class="op">=</span> <span class="fl">123456</span></span>
<span><span class="op">)</span></span></code></pre></div>
    </div>

    <div class="section level2">
    <h2 id="arguments">Arguments<a class="anchor" aria-label="anchor" href="#arguments"></a></h2>


<dl><dt id="arg-texto">texto<a class="anchor" aria-label="anchor" href="#arg-texto"></a></dt>
<dd><p>Texto a analizar. Puede ser una noticia, tweet, documento, etc.</p></dd>


<dt id="arg-instrucciones">instrucciones<a class="anchor" aria-label="anchor" href="#arg-instrucciones"></a></dt>
<dd><p>Instrucciones en lenguaje natural que indican al modelo que hacer
con el texto. Ejemplo: "Extrae todas las entidades nombradas", "Clasifica el sentimiento".</p></dd>


<dt id="arg-modelo">modelo<a class="anchor" aria-label="anchor" href="#arg-modelo"></a></dt>
<dd><p>Modelo de Ollama a utilizar.
- Para Ollama local: "qwen3:1.7b", "llama3.2:latest", "mistral", "phi3", "gemma2"
  (debe estar previamente descargado con `ollama pull nombre_modelo`)
- Para Ollama Cloud API: modelos cloud especificos disponibles sin GPU local:
  "deepseek-v3.1:671b-cloud", "gpt-oss:20b-cloud", "gpt-oss:120b-cloud",
  "kimi-k2:1t-cloud", "qwen3-coder:480b-cloud", "glm-4.6:cloud", "minimax-m2:cloud"
Por defecto: "qwen3:1.7b"</p></dd>


<dt id="arg-schema">schema<a class="anchor" aria-label="anchor" href="#arg-schema"></a></dt>
<dd><p>Esquema JSON que define la estructura de la respuesta. Puede usar
`acep_gpt_schema()` para obtener esquemas predefinidos o crear uno personalizado.
Si es NULL, usa un esquema simple con campo "respuesta".</p></dd>


<dt id="arg-parse-json">parse_json<a class="anchor" aria-label="anchor" href="#arg-parse-json"></a></dt>
<dd><p>Logico. Si TRUE (por defecto), parsea automaticamente el JSON
a un objeto R (lista o data frame). Si FALSE, devuelve el JSON como string.</p></dd>


<dt id="arg-temperature">temperature<a class="anchor" aria-label="anchor" href="#arg-temperature"></a></dt>
<dd><p>Parametro de temperatura (0-2). Valores bajos (0-0.3) generan
respuestas mas deterministas. Valores altos (0.7-1) mas creativas. Por defecto: 0.</p></dd>


<dt id="arg-max-tokens">max_tokens<a class="anchor" aria-label="anchor" href="#arg-max-tokens"></a></dt>
<dd><p>Numero maximo de tokens en la respuesta. Por defecto: 4000.</p></dd>


<dt id="arg-host">host<a class="anchor" aria-label="anchor" href="#arg-host"></a></dt>
<dd><p>URL del servidor Ollama. Por defecto: "http://localhost:11434" para uso local.
Para usar Ollama Cloud API, especificar "https://ollama.com" (sin /api, se agrega automaticamente).</p></dd>


<dt id="arg-api-key">api_key<a class="anchor" aria-label="anchor" href="#arg-api-key"></a></dt>
<dd><p>API key para Ollama API remota. Solo requerido si usas un servidor remoto.
Por defecto busca la variable de entorno OLLAMA_API_KEY. Para uso local (localhost) no es necesario.</p></dd>


<dt id="arg-seed">seed<a class="anchor" aria-label="anchor" href="#arg-seed"></a></dt>
<dd><p>Semilla numerica para reproducibilidad. Por defecto: 123456.</p></dd>

</dl></div>
    <div class="section level2">
    <h2 id="value">Value<a class="anchor" aria-label="anchor" href="#value"></a></h2>
    <p>Si parse_json=TRUE, devuelve una lista o data frame con la respuesta
  estructurada segun el esquema. Si parse_json=FALSE, devuelve un string JSON.</p>
    </div>

    <div class="section level2">
    <h2 id="ref-examples">Examples<a class="anchor" aria-label="anchor" href="#ref-examples"></a></h2>
    <div class="sourceCode"><pre class="sourceCode r"><code><span class="r-in"><span><span class="kw">if</span> <span class="op">(</span><span class="cn">FALSE</span><span class="op">)</span> <span class="op">{</span> <span class="co"># \dontrun{</span></span></span>
<span class="r-in"><span><span class="co"># Primero, instalar Ollama y descargar un modelo:</span></span></span>
<span class="r-in"><span><span class="co"># Terminal: ollama pull llama3.1</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Extraer entidades de un texto</span></span></span>
<span class="r-in"><span><span class="va">texto</span> <span class="op">&lt;-</span> <span class="st">"El SUTEBA convoco a un paro en Buenos Aires el 15 de marzo."</span></span></span>
<span class="r-in"><span><span class="va">instrucciones</span> <span class="op">&lt;-</span> <span class="st">"Extrae todas las entidades nombradas del texto."</span></span></span>
<span class="r-in"><span><span class="va">schema</span> <span class="op">&lt;-</span> <span class="fu"><a href="acep_gpt_schema.html">acep_gpt_schema</a></span><span class="op">(</span><span class="st">"extraccion_entidades"</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="va">resultado</span> <span class="op">&lt;-</span> <span class="fu">acep_ollama</span><span class="op">(</span><span class="va">texto</span>, <span class="va">instrucciones</span>, schema <span class="op">=</span> <span class="va">schema</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">resultado</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Analisis de sentimiento</span></span></span>
<span class="r-in"><span><span class="va">texto</span> <span class="op">&lt;-</span> <span class="st">"La protesta fue pacifica y bien organizada."</span></span></span>
<span class="r-in"><span><span class="va">schema</span> <span class="op">&lt;-</span> <span class="fu"><a href="acep_gpt_schema.html">acep_gpt_schema</a></span><span class="op">(</span><span class="st">"sentimiento"</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="va">resultado</span> <span class="op">&lt;-</span> <span class="fu">acep_ollama</span><span class="op">(</span><span class="va">texto</span>, <span class="st">"Analiza el sentimiento del texto"</span>, schema <span class="op">=</span> <span class="va">schema</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">resultado</span><span class="op">$</span><span class="va">sentimiento_general</span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="co"># Usar Ollama Cloud API (requiere API key)</span></span></span>
<span class="r-in"><span><span class="co"># Los modelos cloud se ejecutan sin necesidad de GPU local</span></span></span>
<span class="r-in"><span><span class="fu"><a href="https://rdrr.io/r/base/Sys.setenv.html" class="external-link">Sys.setenv</a></span><span class="op">(</span>OLLAMA_API_KEY <span class="op">=</span> <span class="st">"tu-api-key"</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="va">resultado_remoto</span> <span class="op">&lt;-</span> <span class="fu">acep_ollama</span><span class="op">(</span></span></span>
<span class="r-in"><span>  texto <span class="op">=</span> <span class="va">texto</span>,</span></span>
<span class="r-in"><span>  instrucciones <span class="op">=</span> <span class="st">"Extrae entidades"</span>,</span></span>
<span class="r-in"><span>  modelo <span class="op">=</span> <span class="st">"deepseek-v3.1:671b-cloud"</span>,  <span class="co"># Modelo cloud de 671B parametros</span></span></span>
<span class="r-in"><span>  host <span class="op">=</span> <span class="st">"https://ollama.com"</span>,</span></span>
<span class="r-in"><span>  schema <span class="op">=</span> <span class="fu"><a href="acep_gpt_schema.html">acep_gpt_schema</a></span><span class="op">(</span><span class="st">"extraccion_entidades"</span><span class="op">)</span></span></span>
<span class="r-in"><span><span class="op">)</span></span></span>
<span class="r-in"><span></span></span>
<span class="r-in"><span><span class="op">}</span> <span class="co"># }</span></span></span>
</code></pre></div>
    </div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside></div>


    <footer><div class="pkgdown-footer-left">
  <p>Developed by Agustín Nieto.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.2.0.</p>
</div>

    </footer></div>





  </body></html>

